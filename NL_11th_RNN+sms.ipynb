{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41bd39db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dcafb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.4.0) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.4.0) (4.61.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.4.0) (1.19.5)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.4.0) (2.26.0)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.4.0) (1.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.4.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.4.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.4.0) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.4.0) (1.26.7)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f7ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea50e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ffbaf",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a1a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 128\n",
    "num_epochs= 10\n",
    "\n",
    "word_vec_size= 256 #==embedding size\n",
    "dropout_p=0.3\n",
    "\n",
    "hidden_size= 512\n",
    "num_layer= 4\n",
    "\n",
    "learning_rate= 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9439170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c66d6",
   "metadata": {},
   "source": [
    "## 1. SMS train, test dataset 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0eb8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a22907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders= DataLoader(\n",
    "    train_fn='data/sms.maxlen.uniq.shuf.train.tsv',\n",
    "    batch_size=batch_size,\n",
    "    valid_ratio= .2,\n",
    "    device=-1,\n",
    "    max_vocab= 999999,\n",
    "    min_freq=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70113775",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaders= DataLoader(\n",
    "    train_fn='data/sms.maxlen.uniq.shuf.test.tsv',\n",
    "    batch_size=batch_size,\n",
    "    valid_ratio= .01,\n",
    "    device=-1,\n",
    "    max_vocab= 999999,\n",
    "    min_freq=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aba062",
   "metadata": {},
   "source": [
    "## 2. 대략적인 데이터 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17ef454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|train| =  3723 |valid| =  931\n",
      "|vocab| = 1541 |classes| = 2\n"
     ]
    }
   ],
   "source": [
    "print(\"|train| = \", len(loaders.train_loader.dataset),\n",
    "     '|valid| = ', len(loaders.valid_loader.dataset))\n",
    "vocab_size= len(loaders.text.vocab)\n",
    "num_classes= len(loaders.label.vocab)\n",
    "print('|vocab| =', vocab_size, '|classes| =', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70adb4f",
   "metadata": {},
   "source": [
    "## 3. 데이터 로드 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ea2304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "한 번에 로드되는 데이터 크기:  128\n",
      "label:  0\n",
      "text:  (12,)\n",
      "label:  0\n",
      "text:  (12,)\n",
      "label:  0\n",
      "text:  (12,)\n",
      "[1]\n",
      "한 번에 로드되는 데이터 크기:  128\n",
      "label:  0\n",
      "text:  (5,)\n",
      "label:  0\n",
      "text:  (5,)\n",
      "label:  0\n",
      "text:  (5,)\n",
      "[2]\n",
      "한 번에 로드되는 데이터 크기:  128\n",
      "label:  0\n",
      "text:  (54,)\n",
      "label:  0\n",
      "text:  (54,)\n",
      "label:  0\n",
      "text:  (54,)\n",
      "[3]\n",
      "한 번에 로드되는 데이터 크기:  128\n",
      "label:  0\n",
      "text:  (31,)\n",
      "label:  1\n",
      "text:  (31,)\n",
      "label:  1\n",
      "text:  (31,)\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "for i, data in enumerate(loaders.train_loader):\n",
    "    labels= data.label\n",
    "    texts= data.text\n",
    "    \n",
    "    if i>n:\n",
    "        break\n",
    "    print('[%d]' %i)\n",
    "    print('한 번에 로드되는 데이터 크기: ', len(labels))\n",
    "    \n",
    "    for j in range(n):\n",
    "#         print(labels[j]) #형태 변환이 어떻게 되는지 궁금해서 출력해봄 -> tensor(0) # 0은 label\n",
    "        label= labels[j].numpy() #tensor -> numpy로 변환\n",
    "        text= texts[j].numpy()\n",
    "        print('label: ', label)\n",
    "        print('text: ', text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d8be5",
   "metadata": {},
   "source": [
    "## 4. 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3acdca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_size, #vocab_size\n",
    "                word_vec_size, #word embbeding vector 차원\n",
    "                hidden_size, #bidirectional LSTM의 hidden state & cell state의 size\n",
    "                n_classes,\n",
    "                num_layers=4, #쌓을 레이어 수\n",
    "                dropout_p= 0.3\n",
    "                ):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size= input_size\n",
    "        self.word_vec_size= word_vec_size\n",
    "        self.hidden_size= hidden_size\n",
    "        self.n_classes= n_classes\n",
    "        self.num_layer= num_layer\n",
    "        self.dropout_p= dropout_p\n",
    "        \n",
    "        #입력차원(vocab_size), 출력차원(word_vec_size)\n",
    "        self.emb= nn.Embedding(input_size, word_vec_size) #부터\n",
    "        \n",
    "        self.lstm= nn.LSTM(input_size= word_vec_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          dropout= dropout_p,\n",
    "                          batch_first=True,\n",
    "                          bidirectional= True)\n",
    "        self.fc= nn.Linear(hidden_size*2, num_classes)\n",
    "        self.activation= nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, length)\n",
    "        x= self.emb(x)\n",
    "        \n",
    "        # x:(batch_size, length, word_vec_size)\n",
    "        x, _= self.lstm(x) # x: output, _: 마지막 time step의 hidden state& cell state\n",
    "        \n",
    "        # x: (batch_size, length, hidden_size*2)\n",
    "        # x[:,-1]: (batch_size, 1, hidden_size*2)\n",
    "        out= self.activation( self.fc(x[:,-1]) ) #마지막 time step\n",
    "        # self.fc(x[:,-1]): (batch_size, num_classes)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a57d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= RNN(input_size= vocab_size,\n",
    "          word_vec_size=word_vec_size,\n",
    "          hidden_size=hidden_size,\n",
    "          n_classes=num_classes,\n",
    "          num_layers=num_layer,\n",
    "          dropout_p=dropout_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e0454b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccr(dloader, imodel):\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    model.eval()\n",
    "    for i, data in enumerate(dloader):\n",
    "        texts= data.text.to(device)\n",
    "        labels= data.label.to(device)\n",
    "        \n",
    "        output= model(texts) #(batch_size, num_classes)\n",
    "        _, output_index= torch.max(output, 1) #(batch_size, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (output_index == labels).sum().float()\n",
    "        \n",
    "    model.train()\n",
    "    return (100*correct/total).numpy() #tensor->numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34cefc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data: 13.53\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Test Data: %.2f\" %ComputeAccr(loaders.valid_loader, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642b9df",
   "metadata": {},
   "source": [
    "## 5. loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85611a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func= nn.NLLLoss()\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e8c8a",
   "metadata": {},
   "source": [
    "## 6. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5de6fec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [1/10], step [10/30], Loss:0.1620, Accr:86.47\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [1/10], step [20/30], Loss:0.4294, Accr:86.25\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [1/10], step [30/30], Loss:0.0864, Accr:86.47\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [2/10], step [10/30], Loss:0.3523, Accr:86.47\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [2/10], step [20/30], Loss:0.1788, Accr:86.47\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [2/10], step [30/30], Loss:0.5501, Accr:86.47\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [3/10], step [10/30], Loss:0.1924, Accr:86.47\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [3/10], step [20/30], Loss:0.1640, Accr:86.47\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [3/10], step [30/30], Loss:0.0991, Accr:86.47\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [4/10], step [10/30], Loss:0.2277, Accr:86.47\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [4/10], step [20/30], Loss:0.1706, Accr:86.47\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [4/10], step [30/30], Loss:1.1701, Accr:86.47\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [5/10], step [10/30], Loss:0.3101, Accr:86.47\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [5/10], step [20/30], Loss:0.6927, Accr:86.47\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [5/10], step [30/30], Loss:0.2042, Accr:86.47\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [6/10], step [10/30], Loss:0.1799, Accr:86.47\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [6/10], step [20/30], Loss:0.7564, Accr:86.47\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [6/10], step [30/30], Loss:0.1347, Accr:86.47\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [7/10], step [10/30], Loss:0.8222, Accr:86.47\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [7/10], step [20/30], Loss:0.6437, Accr:86.47\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [7/10], step [30/30], Loss:0.1415, Accr:86.47\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [8/10], step [10/30], Loss:0.1999, Accr:86.47\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [8/10], step [20/30], Loss:0.8230, Accr:86.47\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [8/10], step [30/30], Loss:0.1400, Accr:86.47\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [9/10], step [10/30], Loss:0.0677, Accr:86.47\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [9/10], step [20/30], Loss:0.2158, Accr:89.69\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [9/10], step [30/30], Loss:0.1236, Accr:85.39\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [10/10], step [10/30], Loss:0.0588, Accr:92.05\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [10/10], step [20/30], Loss:0.4187, Accr:86.36\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [10/10], step [30/30], Loss:0.2630, Accr:92.80\n"
     ]
    }
   ],
   "source": [
    "total_step= len(loaders.train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(loaders.train_loader):\n",
    "        texts= data.text.to(device)\n",
    "        labels= data.label.to(device)\n",
    "\n",
    "        print('[%d]'%i)\n",
    "\n",
    "        # Forward prop\n",
    "        outputs= model(texts)\n",
    "        loss= loss_func(outputs, labels)\n",
    "\n",
    "        # Backward prop & opimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) %10==0:\n",
    "            print('Epoch [{}/{}], step [{}/{}], Loss:{:.4f}, Accr:{:.2f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, total_step, loss.item(), ComputeAccr(loaders.valid_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f075b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data: 92.80\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Test Data: %.2f\" %ComputeAccr(loaders.valid_loader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2528d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "netname= './nets/rnn_weight.pkl'\n",
    "torch.save(model, netname, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6577908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
